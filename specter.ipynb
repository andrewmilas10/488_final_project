{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrewm/store/hf/huggingface/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/andrewm/store/hf/huggingface/'\n",
    "print(os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrewm/store/hf/huggingface/\n",
      "/home/andrewm/store/hf/huggingface/datasets\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.config.HF_CACHE_HOME)\n",
    "print(datasets.config.HF_DATASETS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrewm/store/hf/huggingface/\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "#training/validation/eval metadata\n",
    "configs = ['fos', 'mesh_descriptors', 'cite_count', 'pub_year', 'cite_prediction', 'cite_prediction_new', 'cite_prediction_aug2023refresh', 'high_influence_cite', 'same_author', 'search', 'biomimicry', 'drsm', 'relish', 'nfcorpus', 'peer_review_score_hIndex', 'trec_covid', 'tweet_mentions', 'scidocs_mag_mesh', 'scidocs_view_cite_read', 'paper_reviewer_matching']\n",
    "\n",
    "print(os.environ['HF_HOME'])\n",
    "fos = datasets.load_dataset(\"allenai/scirepeval\", \"fos\")\n",
    "high_influence_cite = datasets.load_dataset(\"allenai/scirepeval\", \"high_influence_cite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    evaluation: Dataset({\n",
      "        features: ['doc_id', 'corpus_id', 'title', 'abstract', 'labels', 'labels_text'],\n",
      "        num_rows: 68147\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['doc_id', 'corpus_id', 'title', 'abstract', 'labels', 'labels_text'],\n",
      "        num_rows: 541218\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['doc_id', 'corpus_id', 'title', 'abstract', 'labels', 'labels_text'],\n",
      "        num_rows: 67631\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    evaluation: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 1199\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 58626\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 7356\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fos)\n",
    "print(high_influence_cite)\n",
    "# print(fos_test)\n",
    "# print(high_influence_cite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos_test = datasets.load_dataset(\"allenai/scirepeval_test\", \"fos\")\n",
    "high_influence_cite_test = datasets.load_dataset(\"allenai/scirepeval_test\", \"high_influence_cite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 6197963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 176430\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 676150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 143686\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "cite_prediction_new = datasets.load_dataset(\"allenai/scirepeval\", \"cite_prediction_new\")\n",
    "print(cite_prediction_new)\n",
    "cite_prediction = datasets.load_dataset(\"allenai/scirepeval\", \"cite_prediction\")\n",
    "print(cite_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'title': 'Detecting tax evasion: a co-evolutionary approach', 'abstract': 'We present an algorithm that can anticipate tax evasion by modeling the co-evolution of tax schemes with auditing policies. Malicious tax non-compliance, or evasion, accounts for billions of lost revenue each year. Unfortunately when tax administrators change the tax laws or auditing procedures to eliminate known fraudulent schemes another potentially more profitable scheme takes it place. Modeling both the tax schemes and auditing policies within a single framework can therefore provide major advantages. In particular we can explore the likely forms of tax schemes in response to changes in audit policies. This can serve as an early warning system to help focus enforcement efforts. In addition, the audit policies can be fine tuned to help improve tax scheme detection. We demonstrate our approach using the iBOB tax scheme and show it can capture the co-evolution between tax evasion and audit policy. Our experiments shows the expected oscillatory behavior of a biological co-evolving system.', 'corpus_id': 18320607}, 'pos': {'title': 'An Intelligent Anti-Money Laundering Method for Detecting Risky Users in the Banking Systems', 'abstract': 'During the last decades, universal economy has experienced money laundering and its destructive impact on the economy of the countries. Money laundering is the process of converting or transferring an asset in order to conceal its illegal source or assist someone that is involved in such crimes. Criminals generally attempt to clean the sources of the funds obtained by crime, using the banking system. Due to the large amount of information in the banks, detecting such behaviors is not feasible without anti-money laundering systems. Money laundering detection is one of the areas, where data mining tools can be useful and effective. In this research, some of the features of the users are extracted from their profiles by studying them. These features may include large financial transactions in risky areas regarding money laundering, reactivation of dormant accounts with considerable amounts, etc. Network training is performed by designing a fuzzy system, developing an adaptive neuro-fuzzy inference system and adding feature vectors of the users to it. The network output can determine the riskiness of the user behavior. The evaluation results reveal that the proposed method increases the accuracy of detecting risky users.', 'corpus_id': 18822522}, 'neg': {'title': 'Adapting nlp and corpus analysis techniques to structured imagery analysis in classical chinese poetry', 'abstract': 'This paper describes some pioneering work as a joint research project between City University of Hong Kong and Yuan Ze University in Taiwan to adapt language resources and technologies in order to set up a computational framework for the study of the creative language employed in classical Chinese poetry. ::: ::: In particular, it will first of all describe an existing ontology of imageries found in poems written during the Tang and the Song dynasties (7th -14th century AD). It will then propose the augmentation of such imageries into primary, complex, extended and textual imageries. A rationale of such a structured approach is that while poets may use a common dichotomy of primary imageries, creative language use is to be found in the creation of complex and compound imageries. This approach will not only support analysis of inter-poets stylistic similarities and differences but will also effectively reveal intra-poet stylistic characteristics. This article will then describe a syntactic parser designed to produce parse trees that will eventually enable the automatic identification of possible imageries and their subsequent structural analysis and classification. Finally, a case study will be presented that investigated the syntactic properties found in two lyrics written by two stylistically different lyric writers in the Song Dynasty.', 'corpus_id': 13573743, 'score': -1}}\n"
     ]
    }
   ],
   "source": [
    "print(cite_prediction_new[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Neither paper cites the other.\n",
      "1: Paper 7133252 (Pos) is cited by paper 2930857 (Query).\n",
      "2: Paper 2769802 (Pos) is cited by paper 1418985 (Query).\n",
      "3: Paper 3367859 (Query) is cited by paper 23585272 (Pos). This citation is influential.\n",
      "4: Paper 10716624 (Pos) is cited by paper 742720 (Query).\n",
      "5: Paper 8528585 (Pos) is cited by paper 744159 (Query).\n",
      "6: Could not retrieve information for one or both papers.\n",
      "7: Paper 5721911 (Query) is cited by paper 21800513 (Pos). This citation is influential.\n",
      "8: Paper 519573 (Query) is cited by paper 4645057 (Pos).\n",
      "9: Paper 88517081 (Pos) is cited by paper 1454065 (Query).\n",
      "10: Paper 997638 (Pos) is cited by paper 153544510 (Query).\n",
      "11: Paper 24615587 (Pos) is cited by paper 15983280 (Query).\n",
      "12: Paper 14623873 (Pos) is cited by paper 7123986 (Query).\n",
      "13: Paper 4579472 (Pos) is cited by paper 3536779 (Query).\n",
      "14: Paper 215827692 (Pos) is cited by paper 162168455 (Query).\n",
      "15: Paper 5439837 (Pos) is cited by paper 5874758 (Query).\n",
      "15: Paper 5874758 (Query) is cited by paper 5439837 (Pos).\n",
      "16: Neither paper cites the other.\n",
      "17: Paper 1285956 (Pos) is cited by paper 1831426 (Query). This citation is influential.\n",
      "18: Paper 35154226 (Pos) is cited by paper 4001212 (Query). This citation is influential.\n",
      "19: Neither paper cites the other.\n",
      "20: Paper 3317933 (Pos) is cited by paper 3079874 (Query).\n",
      "21: Paper 155098414 (Query) is cited by paper 22575683 (Pos).\n",
      "22: Paper 1734620 (Pos) is cited by paper 348228 (Query).\n",
      "23: Paper 3396322 (Query) is cited by paper 23651375 (Pos).\n",
      "24: Paper 13985829 (Pos) is cited by paper 6988667 (Query).\n",
      "25: Paper 12801438 (Pos) is cited by paper 351392 (Query). This citation is influential.\n",
      "26: Paper 118828509 (Pos) is cited by paper 8882403 (Query).\n",
      "27: Paper 3237445 (Query) is cited by paper 29471684 (Pos).\n",
      "28: Paper 158076638 (Query) is cited by paper 15456873 (Pos).\n",
      "29: Paper 20425902 (Pos) is cited by paper 149738 (Query).\n",
      "30: Paper 16631540 (Query) is cited by paper 23767772 (Pos).\n",
      "31: Paper 253260760 (Pos) is cited by paper 1824604 (Query).\n",
      "32: Paper 269834 (Query) is cited by paper 15217667 (Pos).\n",
      "33: Paper 153332985 (Query) is cited by paper 39603824 (Pos).\n",
      "34: Paper 2600307 (Query) is cited by paper 26801448 (Pos).\n",
      "35: Paper 4775863 (Pos) is cited by paper 3475483 (Query).\n",
      "36: Paper 5013862 (Pos) is cited by paper 3565489 (Query).\n",
      "37: Paper 357362 (Query) is cited by paper 207361165 (Pos).\n",
      "38: Paper 2312078 (Query) is cited by paper 123110323 (Pos).\n",
      "39: Paper 590679 (Pos) is cited by paper 254888 (Query).\n",
      "40: Paper 10773407 (Pos) is cited by paper 201285 (Query).\n",
      "41: Paper 151563 (Query) is cited by paper 6001204 (Pos). This citation is influential.\n",
      "42: Paper 2240647 (Pos) is cited by paper 2905305 (Query).\n",
      "43: Paper 3271405 (Query) is cited by paper 30881382 (Pos).\n",
      "44: Paper 57964 (Query) is cited by paper 21072751 (Pos).\n",
      "45: Paper 779920 (Pos) is cited by paper 721970 (Query).\n",
      "46: Paper 42129070 (Pos) is cited by paper 1549789 (Query).\n",
      "47: Paper 221864747 (Pos) is cited by paper 155614806 (Query).\n",
      "48: Paper 152930407 (Query) is cited by paper 153925238 (Pos).\n",
      "49: Paper 152884632 (Query) is cited by paper 75489746 (Pos).\n",
      "50: Paper 2554000 (Query) is cited by paper 45895300 (Pos).\n",
      "51: Paper 216056328 (Pos) is cited by paper 5033396 (Query). This citation is influential.\n",
      "52: Paper 233915966 (Pos) is cited by paper 2082052 (Query).\n",
      "53: Neither paper cites the other.\n",
      "54: Paper 13199034 (Pos) is cited by paper 3051420 (Query).\n",
      "55: Paper 539141 (Query) is cited by paper 823474 (Pos).\n",
      "56: Paper 839789 (Query) is cited by paper 9712904 (Pos).\n",
      "57: Paper 7179057 (Query) is cited by paper 36853336 (Pos).\n",
      "58: Paper 2622726 (Query) is cited by paper 23615317 (Pos).\n",
      "59: Paper 2247272 (Query) is cited by paper 5577505 (Pos).\n",
      "60: Paper 537825 (Query) is cited by paper 17238892 (Pos).\n",
      "61: Paper 1991656 (Query) is cited by paper 20883768 (Pos).\n",
      "62: Paper 77804 (Pos) is cited by paper 737011 (Query).\n",
      "63: Paper 1414089 (Query) is cited by paper 6858953 (Pos).\n",
      "64: Neither paper cites the other.\n",
      "65: Paper 11285671 (Pos) is cited by paper 469179 (Query).\n",
      "66: Paper 1523852 (Query) is cited by paper 19142116 (Pos).\n",
      "67: Paper 239616540 (Pos) is cited by paper 3544392 (Query).\n",
      "68: Paper 952678 (Query) is cited by paper 28222730 (Pos).\n",
      "69: Paper 1387392 (Pos) is cited by paper 3509341 (Query).\n",
      "70: Paper 12053686 (Pos) is cited by paper 4340015 (Query).\n",
      "71: Paper 1125775 (Query) is cited by paper 13972231 (Pos).\n",
      "72: Neither paper cites the other.\n",
      "73: Paper 13987504 (Pos) is cited by paper 2411548 (Query). This citation is influential.\n",
      "74: Paper 542927 (Query) is cited by paper 18487661 (Pos).\n",
      "75: Paper 638647 (Query) is cited by paper 24217338 (Pos).\n",
      "76: Paper 3291540 (Pos) is cited by paper 14698311 (Query).\n",
      "77: Paper 12364629 (Query) is cited by paper 6390222 (Pos).\n",
      "78: Paper 153223639 (Query) is cited by paper 115828623 (Pos).\n",
      "79: Paper 452690 (Pos) is cited by paper 809030 (Query).\n",
      "80: Paper 155364599 (Query) is cited by paper 154610511 (Pos). This citation is influential.\n",
      "81: Paper 208133 (Query) is cited by paper 27938319 (Pos).\n",
      "82: Neither paper cites the other.\n",
      "83: Paper 1610833 (Pos) is cited by paper 2654381 (Query).\n",
      "84: Paper 218649288 (Pos) is cited by paper 2533057 (Query).\n",
      "85: Paper 23763814 (Pos) is cited by paper 2658479 (Query).\n",
      "86: Paper 1453671 (Query) is cited by paper 44047494 (Pos).\n",
      "87: Paper 3422347 (Query) is cited by paper 4948886 (Pos).\n",
      "88: Paper 831921 (Query) is cited by paper 41043306 (Pos).\n",
      "89: Paper 22362042 (Pos) is cited by paper 845458 (Query).\n",
      "90: Paper 4022856 (Pos) is cited by paper 1497115 (Query).\n",
      "91: Paper 254230103 (Pos) is cited by paper 3615360 (Query).\n",
      "92: Paper 3184456 (Query) is cited by paper 6404761 (Pos).\n",
      "93: Paper 154207781 (Query) is cited by paper 15985233 (Pos).\n",
      "94: Paper 13091226 (Pos) is cited by paper 4646557 (Query).\n",
      "95: Paper 157458145 (Query) is cited by paper 154322734 (Pos).\n",
      "96: Paper 21625612 (Pos) is cited by paper 434710 (Query).\n",
      "97: Paper 3215926 (Pos) is cited by paper 759903 (Query).\n",
      "98: Paper 373050 (Pos) is cited by paper 896107 (Query).\n",
      "99: Paper 2497603 (Query) is cited by paper 12101463 (Pos).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'bool' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andrewm/store/specter.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: Could not retrieve information for one or both papers.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39;49m(influential))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'bool' has no len()"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_paper_info(paper_id):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/CorpusId:{paper_id}\"\n",
    "    response = requests.get(url, headers={'X-API-KEY': \"YwS0Vy4Ix91BcoAYYAk9L2XV1IMyUD3c7nsR8rPe\"})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        time.sleep(60*10)\n",
    "        print(f\"Got error {response}\\n\\nRetrying in 10 minutes\")\n",
    "        return get_paper_info(paper_id)\n",
    "\n",
    "def check_citation(source_paper_info, target_paper_id):\n",
    "    citations = source_paper_info.get('citations', [])\n",
    "    for citation in citations:\n",
    "        if citation.get('paperId') == target_paper_id:\n",
    "            return citation.get('isInfluential', False), True\n",
    "    return False, False\n",
    "\n",
    "influentials = []\n",
    "for i in range(5):  # Loop through the first 100 elements\n",
    "    query_paper_id = cite_prediction_new[\"train\"][i]['query']['corpus_id']\n",
    "    pos_paper_id = cite_prediction_new[\"train\"][i]['pos']['corpus_id']\n",
    "\n",
    "    query_paper_info = get_paper_info(query_paper_id)\n",
    "    pos_paper_info = get_paper_info(pos_paper_id)\n",
    "\n",
    "    if query_paper_info and pos_paper_info:\n",
    "        query_paper_paperId = query_paper_info.get('paperId')\n",
    "        pos_paper_paperId = pos_paper_info.get('paperId')\n",
    "\n",
    "        influential, found1 = check_citation(query_paper_info, pos_paper_paperId)\n",
    "        if found1:\n",
    "            print(f\"{i}: Paper {pos_paper_id} (Pos) is cited by paper {query_paper_id} (Query).\", end='')\n",
    "            if influential:\n",
    "                print(\" This citation is influential.\")\n",
    "                influentials.append((i, 0))\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "        influential, found2 = check_citation(pos_paper_info, query_paper_paperId)\n",
    "        if found2:\n",
    "            print(f\"{i}: Paper {query_paper_id} (Query) is cited by paper {pos_paper_id} (Pos).\", end='')\n",
    "            if influential:\n",
    "                print(\" This citation is influential.\")\n",
    "                influentials.append((i, 1))\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "        if not found1 and not found2:\n",
    "            print(f\"{i}: Neither paper cites the other.\")\n",
    "    else:\n",
    "        print(f\"{i}: Could not retrieve information for one or both papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (7, 1), (17, 0), (18, 0), (25, 0), (41, 1), (51, 0), (73, 0), (80, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(influentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['query_id', 'cand_id', 'score'],\n",
      "        num_rows: 58255\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "high_influence_cite_test = datasets.load_dataset(\"allenai/scirepeval_test\", \"high_influence_cite\")\n",
    "print(high_influence_cite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    evaluation: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 1199\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 58626\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'candidates'],\n",
      "        num_rows: 7356\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "high_influence_cite = datasets.load_dataset(\"allenai/scirepeval\", \"high_influence_cite\")\n",
    "print(high_influence_cite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6197963\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andrewm/store/specter.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m                                 pickle\u001b[39m.\u001b[39mdump({\u001b[39m'\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m'\u001b[39m: results, \u001b[39m'\u001b[39m\u001b[39mlast_iteration\u001b[39m\u001b[39m'\u001b[39m: i}, file)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m         elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Elapsed Time: \u001b[39m\u001b[39m{\u001b[39;00melapsed_time\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds, Entries: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(results)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.229.196/home/andrewm/store/specter.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# results now contains the required list of objects\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def get_paper_info(paper_id, is_corpus_id = False, retry_attempts=5):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{'CorpusId:' if is_corpus_id else ''}{paper_id}\"\n",
    "    headers = {'X-API-KEY': \"YwS0Vy4Ix91BcoAYYAk9L2XV1IMyUD3c7nsR8rPe\"}\n",
    "\n",
    "    for attempt in range(retry_attempts):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Gots error {response}\\n\\nRetrying in 30 seconds\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    # Return None if all retry attempts fail\n",
    "    return None\n",
    "\n",
    "def find_influential_citation(paper_info):\n",
    "    influential_citations = [citation for citation in paper_info.get('citations', []) if citation.get('isInfluential', False)]\n",
    "    if influential_citations:\n",
    "        return random.choice(influential_citations)\n",
    "    return None\n",
    "\n",
    "def find_random_reference(paper_info, exclude_id):\n",
    "    references = [reference for reference in paper_info.get('references', []) if reference.get('paperId') != exclude_id]\n",
    "    if references:\n",
    "        return random.choice(references)\n",
    "    return None\n",
    "\n",
    "def is_influential_citation(source_paper_info, target_paper_id):\n",
    "    for citation in source_paper_info.get('citations', []):\n",
    "        if citation.get('paperId') == target_paper_id and citation.get('isInfluential', False):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "random.seed(42)\n",
    "results = []\n",
    "\n",
    "print(len(cite_prediction_new[\"train\"]))\n",
    "start_time = time.time() \n",
    "\n",
    "for i in range(len(cite_prediction_new[\"train\"])):\n",
    "    query_corpus_id = cite_prediction_new[\"train\"][i]['query']['corpus_id']\n",
    "    query_paper_info = get_paper_info(query_corpus_id, is_corpus_id = True)\n",
    "    if query_paper_info:\n",
    "        influential_citation = find_influential_citation(query_paper_info)\n",
    "        if influential_citation:\n",
    "            new_q_paper_info = get_paper_info(influential_citation.get('paperId'))\n",
    "            if new_q_paper_info and 'abstract' in new_q_paper_info and any(ref.get('paperId') == query_paper_info.get('paperId') for ref in new_q_paper_info.get('references', [])):\n",
    "                neg_paper = find_random_reference(new_q_paper_info, query_paper_info.get('paperId'))\n",
    "                if neg_paper:\n",
    "                    neg_paper_info = get_paper_info(neg_paper.get('paperId'))\n",
    "                    if neg_paper and not is_influential_citation(neg_paper_info, new_q_paper_info.get('paperId')):\n",
    "                        if new_q_paper_info.get('abstract') and neg_paper_info.get('abstract'):\n",
    "                            results.append({\n",
    "                                'query': {\n",
    "                                    'title': new_q_paper_info.get('title'),\n",
    "                                    'abstract': new_q_paper_info.get('abstract'),\n",
    "                                    'corpus_id': new_q_paper_info.get('corpusId')\n",
    "                                },\n",
    "                                'pos': {\n",
    "                                    'title': query_paper_info.get('title'),\n",
    "                                    'abstract': query_paper_info.get('abstract'),\n",
    "                                    'corpus_id': query_paper_info.get('corpusId')\n",
    "                                },\n",
    "                                'neg': {\n",
    "                                    'title': neg_paper_info.get('title'),\n",
    "                                    'abstract': neg_paper_info.get('abstract'),\n",
    "                                    'corpus_id': neg_paper_info.get('corpusId'),\n",
    "                                    'score': -1\n",
    "                                }\n",
    "                            })\n",
    "                            # Pickling the data\n",
    "                            with open('results_dataset.pkl', 'wb') as file:\n",
    "                                pickle.dump({'results': results, 'last_iteration': i}, file)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Iteration: {i + 1}, Elapsed Time: {elapsed_time:.2f} seconds, Entries: {len(results)}\")\n",
    "\n",
    "# results now contains the required list of objects\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Iteration: 24365\n",
      "Total Entries: 5547\n",
      "{'query': {'title': 'Image Authentication Using Self-Supervised Learning to Detect Manipulation Over Social Network Platforms', 'abstract': \"Social media nowadays has a direct impact on people's daily lives as many edge devices are available at our disposal and controlled by our fingertips. With such advancement in communication technology comes a rapid increase of disinformation in many kinds and shapes; faked images are one of the primary examples of misinformation media that can affect many users. Such activity can severely impact public behavior, attitude, and belief or sway the viewers' perception in any malicious or benign direction. Mitigating such disinformation over the Internet is becoming an issue with increasing interest from many aspects of our society, and effective authentication for detecting manipulated images has become extremely important. Perceptual hashing (pHash) is one of the effective techniques for detecting image manipulations. This paper develops a new and a robust pHash authentication approach to detect fake imagery on social media networks, choosing Facebook and Twitter as case studies. Our proposed pHash utilizes a self-supervised learning framework and contrastive loss. In addition, we develop a fake image sample generator in the pre-processing stage to cover the three most known image attacks (copy-move, splicing, and removal). The proposed authentication technique outperforms state-of-the-art pHash methods based on the SMPI dataset and other similar datasets that target one or more image attacks types.\", 'corpus_id': 254089476}, 'pos': {'title': 'A Visual Model-Based Perceptual Image Hash for Content Authentication', 'abstract': \"Perceptual image hash has been widely investigated in an attempt to solve the problems of image content authentication and content-based image retrieval. In this paper, we combine statistical analysis methods and visual perception theory to develop a real perceptual image hash method for content authentication. To achieve real perceptual robustness and perceptual sensitivity, the proposed method uses Watson's visual model to extract visually sensitive features that play an important role in the process of humans perceiving image content. We then generate robust perceptual hash code by combining image-block-based features and key-point-based features. The proposed method achieves a tradeoff between perceptual robustness to tolerate content-preserving manipulations and a wide range of geometric distortions and perceptual sensitivity to detect malicious tampering. Furthermore, it has the functionality to detect compromised image regions. Compared with state-of-the-art schemes, the proposed method obtains a better comprehensive performance in content-based image tampering detection and localization.\", 'corpus_id': 15983280}, 'neg': {'title': 'TransForensics: Image Forgery Localization with Dense Self-Attention', 'abstract': 'Nowadays advanced image editing tools and technical skills produce tampered images more realistically, which can easily evade image forensic systems and make authenticity verification of images more difficult. To tackle this challenging problem, we introduce TransForensics, a novel image forgery localization method inspired by Transformers. The two major components in our framework are dense self-attention encoders and dense correction modules. The former is to model global context and all pairwise inter-actions between local patches at different scales, while the latter is used for improving the transparency of the hidden layers and correcting the outputs from different branches. Compared to previous traditional and deep learning methods, TransForensics not only can capture discriminative representations and obtain high-quality mask predictions but is also not limited by tampering types and patch sequence orders. By conducting experiments on main bench-marks, we show that TransForensics outperforms the state-of-the-art methods by a large margin.', 'corpus_id': 236957018, 'score': -1}}\n",
      "{'query': {'title': 'Plant Products as Antimicrobial Agents', 'abstract': 'SUMMARY The use of and search for drugs and dietary supplements derived from plants have accelerated in recent years. Ethnopharmacologists, botanists, microbiologists, and natural-products chemists are combing the Earth for phytochemicals and “leads” which could be developed for treatment of infectious diseases. While 25 to 50% of current pharmaceuticals are derived from plants, none are used as antimicrobials. Traditional healers have long used plants to prevent or cure infectious conditions; Western medicine is trying to duplicate their successes. Plants are rich in a wide variety of secondary metabolites, such as tannins, terpenoids, alkaloids, and flavonoids, which have been found in vitro to have antimicrobial properties. This review attempts to summarize the current status of botanical screening efforts, as well as in vivo studies of their effectiveness and toxicity. The structure and antimicrobial properties of phytochemicals are also addressed. Since many of these compounds are currently available as unregulated botanical preparations and their use by the public is increasing rapidly, clinicians need to consider the consequences of patients self-medicating with these preparations.', 'corpus_id': 12801438}, 'pos': {'title': 'Therapeutic efficacy of Ashwagandha against experimental aspergillosis in mice.', 'abstract': 'Therapeutic efficacy of an Indian Ayurvedic medicinal preparation, Ashwagandha [Withania somnifera L. Dunal (Solanceae; root)] was evaluated against experimental aspergillosis in Balb/c mice. Ashwagandha given orally once daily for 7 consecutive days in a dose of 100 mg/kg after intravenous infection of Aspergillus fumigatus prolonged the survival period of infected mice. This protective activity was probably related to the observed increases in phagocytosis and intracellular killing of peritoneal macrophages induced by Ashwaganda treatment. The number of peripheral leukocytes was not modified, excluding a possibility of mobilization of cells from other compartments. On the basis of these findings, the probable mechanism underlying the protective action of Ashwagandha against systemic Aspergillus infection was discussed in relation with its possible activity to activate the macrophage function.', 'corpus_id': 351392}, 'neg': {'title': 'Indonesian medicinal plants. XVII. Characterization of quassinoids from the stems of Quassia indica.', 'abstract': 'Four new quassinoids named samaderines X (1), Y (2) and Z (3), and indaquassin X (5), and a new C19 quassinoid glycoside, 2-O-glucosylsamaderine C (10), together with five known quassinoids, samaderines B (7), C (8), and E (4), indaquassin C (6), and simarinolide (9), were isolated form the stems of Quassia indica (Simaroubaceae), an Indonesian medicinal plant. The chemical structures of these quassinoids have been elucidated on the bases of their chemical and physiochemical properties. Samaderines X (1), Z (3), E (4), and B (7) were shown to exhibit significant growth-inhibitory activity against the cultured malarial parasite Plasmodium falciparum (a chloroquine- resistant K1 strain), and 1--8 were shown to exhibit in vitro cytotoxicity (IC50: 0.04--100 micrograms/ml) against KB cells. Samaderines X (1), B (7) and C (8), as well s indaquassin X (5), exhibited inhibitory activity in the in vitro endothelial cell-neutrophil leukocyte adhesion assay, whereas samaderines X (1) and B (7) were found to exhibit significant anti-inflammatory activity.', 'corpus_id': 11353176, 'score': -1}}\n",
      "{'query': {'title': 'Ambroxol Hydrochloride Loaded Gastro-Retentive Nanosuspension Gels Potentiate Anticancer Activity in Lung Cancer (A549) Cells', 'abstract': 'This study aimed to develop gastro-retentive sustained-release ambroxol (ABX) nanosuspensions utilizing ambroxol-kappa-carrageenan (ABX-CRGK) complexation formulations. The complex was characterized by differential scanning calorimetry, powder x-ray diffractometer, and scanning electron microscopy. The prepared co-precipitate complex was used for the development of the sustained-release formulation to overcome the high metabolic and poor solubility problems associated with ABX. Furthermore, the co-precipitate complex was formulated as a suspension in an aqueous floating gel-forming vehicle of sodium alginate with chitosan, which might be beneficial for targeting the stomach as a good absorption site for ABX. The suspension exhibited rapid floating gel behaviour for more than 8 h, thus confirming the gastro-retentive effects. Particle size analysis revealed that the optimum nanosuspension (ABX-NS) had a mean particle size of 332.3 nm. Afterward, the ABX released by the nanoparticles would be distributed to the pulmonary tissue as previously described. Based on extensive pulmonary distribution, the developed nanosuspension-released ABX nanoparticles showed significant cytotoxic enhancement compared to free ABX in A549 lung cancer cells. However, a significant loss of mitochondrial membrane potential (MMP) also occurred. The level of caspase-3 was the highest in the ABX-NS-released particle-treated samples, with a value of 416.6 ± 9.11 pg/mL. Meanwhile, the levels of nuclear factor kappa beta, interleukins 6 and 1 beta, and tumour necrosis alpha (NF-kB, IL-6, IL-1β, and TNF-α, respectively) were lower for ABX-NS compared to free ABX (p < 0.05). In caspase-3, Bax, and p53, levels significantly increased in the presence of ABX-NS compared to free ABX. Overall, ABX-NS produced an enhancement of the anticancer effects of ABX on the A549 cells, and the developed sustained-release gel was successful in providing a gastro-retentive effect.', 'corpus_id': 244824065}, 'pos': {'title': 'Biodegradable Chitosan-Based Ambroxol Hydrochloride Microspheres: Effect of Cross-Linking Agents', 'abstract': 'The objective of this study was to investigate the influence of type of cross-linking method used on the properties of ambroxol hydrochloride microspheres such as encapsulation efficiency, particle size, and drug release. Microspheres were prepared by solvent evaporation technique using chitosan as a matrix-forming agent and cross-linked using formaldehyde and heat treatment. Morphological and physicochemical properties of microspheres were then investigated by scanning electron microscopy (SEM), X-ray diffractometry (XRD), differential scanning calorimetry (DSC), and Fourier-transform infrared spectroscopy (FTIR) spectroscopy. The cross-linking of chitosan takes place at the free amino group because of formation of imine bond as evidenced by FTIR. The DSC, XRD, and FTIR analysis showed that chitosan microspheres cross linked by heating were superior in properties and performance as compared to the microspheres cross-linked using formaldehyde. SEM results revealed that heat-treated microspheres were spherical, discrete having smooth, and porous structure. The particle size and encapsulation efficiencies of the prepared chitosan microspheres ranged between 10.83-24.11 μm and 39.73-80.56%, respectively. The drug release was extended up to 12 h, and the kinetics of the drug release was obeying Higuchi kinetic proving diffusion-controlled drug release.', 'corpus_id': 16631540}, 'neg': {'title': 'Anticancer Effects of Sinulariolide-Conjugated Hyaluronan Nanoparticles on Lung Adenocarcinoma Cells', 'abstract': 'Lung cancer is one of the most clinically challenging malignant diseases worldwide. Sinulariolide (SNL), extracted from the farmed coral species Sinularia flexibilis, has been used for suppressing malignant cells. For developing anticancer therapeutic agents, we aimed to find an alternative for non-small cell lung cancer treatment by using SNL as the target drug. We investigated the SNL bioactivity on A549 lung cancer cells by conjugating SNL with hyaluronan nanoparticles to form HA/SNL aggregates by using a high-voltage electrostatic field system. SNL was toxic on A549 cells with an IC50 of 75 µg/mL. The anticancer effects of HA/SNL aggregates were assessed through cell viability assay, apoptosis assays, cell cycle analyses, and western blotting. The size of HA/SNL aggregates was approximately 33–77 nm in diameter with a thin continuous layer after aggregating numerous HA nanoparticles. Flow cytometric analysis revealed that the HA/SNL aggregate-induced apoptosis was more effective at a lower SNL dose of 25 µg/mL than pure SNL. Western blotting indicated that caspases-3, -8, and -9 and Bcl-xL and Bax played crucial roles in the apoptotic signal transduction pathway. In summary, HA/SNL aggregates exerted stronger anticancer effects on A549 cells than did pure SNL via mitochondria-related pathways.', 'corpus_id': 6136612, 'score': -1}}\n",
      "{'query': {'title': 'Automated MRI segmentation for individualized modeling of current flow in the human head', 'abstract': 'Objective. High-definition transcranial direct current stimulation (HD-tDCS) and high-density electroencephalography require accurate models of current flow for precise targeting and current source reconstruction. At a minimum, such modeling must capture the idiosyncratic anatomy of the brain, cerebrospinal fluid (CSF) and skull for each individual subject. Currently, the process to build such high-resolution individualized models from structural magnetic resonance images requires labor-intensive manual segmentation, even when utilizing available automated segmentation tools. Also, accurate placement of many high-density electrodes on an individual scalp is a tedious procedure. The goal was to develop fully automated techniques to reduce the manual effort in such a modeling process. Approach. A fully automated segmentation technique based on Statical Parametric Mapping 8, including an improved tissue probability map and an automated correction routine for segmentation errors, was developed, along with an automated electrode placement tool for high-density arrays. The performance of these automated routines was evaluated against results from manual segmentation on four healthy subjects and seven stroke patients. The criteria include segmentation accuracy, the difference of current flow distributions in resulting HD-tDCS models and the optimized current flow intensities on cortical targets.Main results. The segmentation tool can segment out not just the brain but also provide accurate results for CSF, skull and other soft tissues with a field of view extending to the neck. Compared to manual results, automated segmentation deviates by only 7% and 18% for normal and stroke subjects, respectively. The predicted electric fields in the brain deviate by 12% and 29% respectively, which is well within the variability observed for various modeling choices. Finally, optimized current flow intensities on cortical targets do not differ significantly.Significance. Fully automated individualized modeling may now be feasible for large-sample EEG research studies and tDCS clinical trials.', 'corpus_id': 1920028}, 'pos': {'title': 'The point spread function of the human head and its implications for transcranial current stimulation', 'abstract': 'Rational development of transcranial current stimulation (tCS) requires solving the ‘forward problem’: the computation of the electric field distribution in the head resulting from the application of scalp currents. Derivation of forward models has represented a major effort in brain stimulation research, with model complexity ranging from spherical shells to individualized head models based on magnetic resonance imagery. Despite such effort, an easily accessible benchmark head model is greatly needed when individualized modeling is either undesired (to observe general population trends as opposed to individual differences) or unfeasible. Here, we derive a closed-form linear system which relates the applied current to the induced electric potential. It is shown that in the spherical harmonic (Fourier) domain, a simple scalar multiplication relates the current density on the scalp to the electric potential in the brain. Equivalently, the current density in the head follows as the spherical convolution between the scalp current distribution and the point spread function of the head, which we derive. Thus, if one knows the spherical harmonic representation of the scalp current (i.e. the electrode locations and current intensity to be employed), one can easily compute the resulting electric field at any point inside the head. Conversely, one may also readily determine the scalp current distribution required to generate an arbitrary electric field in the brain (the ‘backward problem’ in tCS). We demonstrate the simplicity and utility of the model with a series of characteristic curves which sweep across a variety of stimulation parameters: electrode size, depth of stimulation, head size and anode–cathode separation. Finally, theoretically optimal montages for targeting an infinitesimal point in the brain are shown.', 'corpus_id': 3565489}, 'neg': {'title': 'A First Course in the Finite Element Method', 'abstract': \"1. INTRODUCTION Brief History. Introduction to Matrix Notation. Role of the Computer. General Steps of the Finite Element Method. Applications of the Finite Element Method. Advantages of the Finite Element Method. Computer Programs for the Finite Element Method. 2. INTRODUCTION TO THE STIFFNESS (DISPLACEMENT) METHOD Definition of the Stiffness Matrix. Derivation of the Stiffness Matrix for a Spring Element. Example of a Spring Assemblage. Assembling the Total Stiffness Matrix by Superposition (Direct Stiffness Method). Boundary Conditions. Potential Energy Approach to Derive Spring Element Equations. 3. DEVELOPMENT OF TRUSS EQUATIONS Derivation of the Stiffness Matrix for a Bar Element in Local Coordinates. Selecting Approximation Functions for Displacements. Transformation of Vectors in Two Dimensions. Global Stiffness Matrix for Bar Arbitrarily Oriented in the Plane. Computation of Stress for a Bar in the x-y Plane. Solution of a Plane Truss. Transformation Matrix and Stiffness Matrix for a Bar in Three-Dimensional Space. Use of Symmetry in Structure. Inclined, or Skewed, Supports. Potential Energy Approach to Derive Bar Element Equations. Comparison of Finite Element Solution to Exact Solution for Bar. Galerkin's Residual Method and Its Use to Derive the One-Dimensional Bar Element Equations. Other Residual Methods and Their Application to a One-Dimensional Bar Problem. Flowchart for Solutions of Three-Dimensional Truss Problems. Computer Program Assisted Step-by-Step Solution for Truss Problem. 4. DEVELOPMENT OF BEAM EQUATIONS Beam Stiffness. Example of Assemblage of Beam Stiffness Matrices. Examples of Beam Analysis Using the Direct Stiffness Method. Distribution Loading. Comparison of the Finite Element Solution to the Exact Solution for a Beam. Beam Element with Nodal Hinge. Potential Energy Approach to Derive Beam Element Equations. Galerkin's Method for Deriving Beam Element Equations. 5. FRAME AND GRID EQUATIONS Two-Dimensional Arbitrarily Oriented Beam Element. Rigid Plane Frame Examples. Inclined or Skewed Supports - Frame Element. Grid Equations. Beam Element Arbitrarily Oriented in Space. Concept of Substructure Analysis. 6. DEVELOPMENT OF THE PLANE STRESS AND STRAIN STIFFNESS EQUATIONS Basic Concepts of Plane Stress and Plane Strain. Derivation of the Constant-Strain Triangular Element Stiffness Matrix and Equations. Treatment of Body and Surface Forces. Explicit Expression for the Constant-Strain Triangle Stiffness Matrix. Finite Element Solution of a Plane Stress Problem. Rectangular Plane Element (Bilinear Rectangle, Q4). 7. PRACTICAL CONSIDERATIONS IN MODELING: INTERPRETING RESULTS AND EXAMPELS OF PLANE STRESS/STRAIN ANALYSIS Finite Element Modeling. Equilibrium and Compatibility of Finite Element Results. Convergence of Solution. Interpretation of Stresses. Static Condensation. Flowchart for the Solution of Plane Stress-Strain Problems. Computer Program Assisted Step-by-Step Solution, Other Models, and Results for Plane Stress-Strain Problems. 8. DEVELOPMENT OF THE LINEAR-STRAIN TRAINGLE EQUATIONS Derivation of the Linear-Strain Triangular Element Stiffness Matrix and Equations. Example of LST Stiffness Determination. Comparison of Elements. 9. AXISYMMETRIC ELEMENTS Derivation of the Stiffness Matrix. Solution of an Axisymmetric Pressure Vessel. Applications of Axisymmetric Elements. 10. ISOPARAMETRIC FORMULATION Isoparametric Formulation of the Bar Element Stiffness Matrix. Isoparametric Formulation of the Okabe Quadrilateral Element Stiffness Matrix. Newton-Cotes and Gaussian Quadrature. Evaluation of the Stiffness Matrix and Stress Matrix by Gaussian Quadrature. Higher-Order Shape Functions. 11. THREE-DIMENSIONAL STRESS ANALYSIS Three-Dimensional Stress and Strain. Tetrahedral Element. Isoparametric Formulation. 12. PLATE BENDING ELEMENT Basic Concepts of Plate Bending. Derivation of a Plate Bending Element Stiffness Matrix and Equations. Some Plate Element Numerical Comparisons. Computer Solutions for Plate Bending Problems. 13. HEAT TRANSFER AND MASS TRANSPORT Derivation of the Basic Differential Equation. Heat Transfer with Convection. Typical Units Thermal Conductivities K and Heat-Transfer Coefficients, h. One-Dimensional Finite Element Formulation Using a Variational Method. Two-Dimensional Finite Element Formulation. Line or Point Sources. Three-Dimensional Heat Transfer by the Finite Element Method. One-Dimensional Heat Transfer with Mass Transport. Finite Element Formulation of Heat Transfer with Mass Transport by Galerkin's Method. Flowchart and Examples of a Heat-Transfer Program. 14. FLUID FLOW IN POROUS MEDIA AND THROUGH HYDRAULIC NETWORKS AND ELECTRICAL NETWORKS AND ELECTROSTATICS Derivation of the Basic Differential Equations. One-Dimensional Finite Element Formulation. Two-Dimensional Finite Element Formulation. Flowchart and Example of a Fluid-Flow Program. Electrical Networks. Electrostatics. 15. THERMAL STRESS Formulation of the Thermal Stress Problem and Examples. 16. STRUCTURAL DYNAMICS AND TIME-DEPENDENT HEAT TRANSFER Dynamics of a Spring-Mass System. Direct Derivation of the Bar Element Equations. Numerical Integration in Time. Natural Frequencies of a One-Dimensional Bar. Time-Dependent One-Dimensional Bar Analysis. Beam Element Mass Matrices and Natural Frequencies. Truss, Plane Frame, Plane Stress, Plane Strain, Axisymmetric, and Solid Element Mass Matrices. Time-Dependent Heat-Transfer. Computer Program Example Solutions for Structural Dynamics. APPENDIX A - MATRIX ALGEBRA Definition of a Matrix. Matrix Operations. Cofactor of Adjoint Method to Determine the Inverse of a Matrix. Inverse of a Matrix by Row Reduction. Properties of Stiffness Matrices. APPENDIX B - METHODS FOR SOLUTION OF SIMULTANEOUS LINEAR EQUATIONS Introduction. General Form of the Equations. Uniqueness, Nonuniqueness, and Nonexistence of Solution. Methods for Solving Linear Algebraic Equations. Banded-Symmetric Matrices, Bandwidth, Skyline, and Wavefront Methods. APPENDIX C - EQUATIONS FOR ELASTICITY THEORY Introduction. Differential Equations of Equilibrium. Strain/Displacement and Compatibility Equations. Stress-Strain Relationships. APPENDIX D - EQUIVALENT NODAL FORCES APPENDIX E - PRINCIPLE OF VIRTUAL WORK APPENDIX F - PROPERTIES OF STRUCTURAL STEEL AND ALUMINUM SHAPES ANSWERS TO SELECTED PROBLEMS INDEX\", 'corpus_id': 117096840, 'score': -1}}\n",
      "{'query': {'title': 'Deep web performance enhance on search engine', 'abstract': \"Due to digital preservation and new generation technology Deep Web increasing faster than Surface Web, it's necessary to public accessible content also retrieving on general search engine. Huge amounts of data like documents, unstructured, distributed, multi-media available on HTML forms or hidden or invisible or difficult to access known as Deep Web has becoming one of the most valuable resources. Surface Web and Deep Web are two types of Web. The traditional or general search engines like - Google, Yahoo, MSN, Bing etc. better crawling Surface Web only whose pages are directly indexed by general search engines. In this paper proposed on Deep Web public access content that hidden data indexing enhance by general search engine crawler. Sitemap, search engine crawler's Robot.txt and meta data (data about data) technique implemented on a particular developed website. Where all documents data store in databases that access by HTML forms. For the result comparing purpose developed similar Deep Web website which has not implements aforesaid technique. Google webmaster tool used for result analysis. Quicker result found on which have technique implemented that is indexed by Google crawler.\", 'corpus_id': 8132007}, 'pos': {'title': 'Probe, count, and classify: categorizing hidden web databases', 'abstract': 'The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.', 'corpus_id': 721970}, 'neg': {'title': 'Truth Finding on the Deep Web: Is the Problem Solved?', 'abstract': \"The amount of useful information available on the Web has been growing at a dramatic pace in recent years and people rely more and more on the Web to fulfill their information needs. In this paper, we study truthfulness of Deep Web data in two domains where we believed data are fairly clean and data quality is important to people's lives: Stock and Flight. To our surprise, we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy. We further applied on these two data sets state-of-the-art data fusion methods that aim at resolving conflicts and finding the truth, analyzed their strengths and limitations, and suggested promising research directions. We wish our study can increase awareness of the seriousness of conflicting data on the Web and in turn inspire more research in our community to tackle this problem.\", 'corpus_id': 3133027, 'score': -1}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in binary read mode\n",
    "with open('results1.pkl', 'rb') as file:\n",
    "    # Load and deserialize the data from the file\n",
    "    data = pickle.load(file)\n",
    "\n",
    "    # Extracting the results and last iteration\n",
    "    results = data['results']\n",
    "    last_iteration = data['last_iteration']\n",
    "\n",
    "    # Print the retrieved data\n",
    "    print(f\"Last Iteration: {last_iteration}\")\n",
    "    print(f\"Total Entries: {len(results)}\")\n",
    "\n",
    "    # Optionally, print the actual data (or part of it)\n",
    "    for entry in results[:5]:  # Print first 5 entries as a sample\n",
    "        print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Iteration: 573856\n",
      "Total Entries: 12261\n",
      "Last Iteration: 545057\n",
      "Total Entries: 11394\n",
      "Last Iteration: 916548\n",
      "Total Entries: 19708\n",
      "Last Iteration: 552939\n",
      "Total Entries: 11715\n",
      "Last Iteration: 559930\n",
      "Total Entries: 12081\n",
      "Last Iteration: 314451\n",
      "Total Entries: 6360\n",
      "Last Iteration: 184432\n",
      "Total Entries: 3565\n",
      "Last Iteration: 564373\n",
      "Total Entries: 12108\n",
      "Last Iteration: 3380354\n",
      "Total Entries: 74942\n",
      "Last Iteration: 443615\n",
      "Total Entries: 9375\n",
      "Total 173509\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "total_entries = 0\n",
    "for i in range(24366, 24376):\n",
    "    with open(f\"results_start_{i}_stride_10.pkl\", 'rb') as file:\n",
    "        # Load and deserialize the data from the file\n",
    "        data = pickle.load(file)\n",
    "\n",
    "        # Extracting the results and last iteration\n",
    "        results = data['results']\n",
    "        last_iteration = data['last_iteration']\n",
    "        total_entries += len(results)\n",
    "\n",
    "        # Print the retrieved data\n",
    "        print(f\"Last Iteration: {last_iteration}\")\n",
    "        print(f\"Total Entries: {len(results)}\")\n",
    "print(\"Total\", total_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179056\n",
      "{'query': {'title': 'Image Authentication Using Self-Supervised Learning to Detect Manipulation Over Social Network Platforms', 'abstract': \"Social media nowadays has a direct impact on people's daily lives as many edge devices are available at our disposal and controlled by our fingertips. With such advancement in communication technology comes a rapid increase of disinformation in many kinds and shapes; faked images are one of the primary examples of misinformation media that can affect many users. Such activity can severely impact public behavior, attitude, and belief or sway the viewers' perception in any malicious or benign direction. Mitigating such disinformation over the Internet is becoming an issue with increasing interest from many aspects of our society, and effective authentication for detecting manipulated images has become extremely important. Perceptual hashing (pHash) is one of the effective techniques for detecting image manipulations. This paper develops a new and a robust pHash authentication approach to detect fake imagery on social media networks, choosing Facebook and Twitter as case studies. Our proposed pHash utilizes a self-supervised learning framework and contrastive loss. In addition, we develop a fake image sample generator in the pre-processing stage to cover the three most known image attacks (copy-move, splicing, and removal). The proposed authentication technique outperforms state-of-the-art pHash methods based on the SMPI dataset and other similar datasets that target one or more image attacks types.\", 'corpus_id': 254089476}, 'pos': {'title': 'A Visual Model-Based Perceptual Image Hash for Content Authentication', 'abstract': \"Perceptual image hash has been widely investigated in an attempt to solve the problems of image content authentication and content-based image retrieval. In this paper, we combine statistical analysis methods and visual perception theory to develop a real perceptual image hash method for content authentication. To achieve real perceptual robustness and perceptual sensitivity, the proposed method uses Watson's visual model to extract visually sensitive features that play an important role in the process of humans perceiving image content. We then generate robust perceptual hash code by combining image-block-based features and key-point-based features. The proposed method achieves a tradeoff between perceptual robustness to tolerate content-preserving manipulations and a wide range of geometric distortions and perceptual sensitivity to detect malicious tampering. Furthermore, it has the functionality to detect compromised image regions. Compared with state-of-the-art schemes, the proposed method obtains a better comprehensive performance in content-based image tampering detection and localization.\", 'corpus_id': 15983280}, 'neg': {'title': 'TransForensics: Image Forgery Localization with Dense Self-Attention', 'abstract': 'Nowadays advanced image editing tools and technical skills produce tampered images more realistically, which can easily evade image forensic systems and make authenticity verification of images more difficult. To tackle this challenging problem, we introduce TransForensics, a novel image forgery localization method inspired by Transformers. The two major components in our framework are dense self-attention encoders and dense correction modules. The former is to model global context and all pairwise inter-actions between local patches at different scales, while the latter is used for improving the transparency of the hidden layers and correcting the outputs from different branches. Compared to previous traditional and deep learning methods, TransForensics not only can capture discriminative representations and obtain high-quality mask predictions but is also not limited by tampering types and patch sequence orders. By conducting experiments on main bench-marks, we show that TransForensics outperforms the state-of-the-art methods by a large margin.', 'corpus_id': 236957018, 'score': -1}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "total_entries = 0\n",
    "results = []\n",
    "\n",
    "with open('results1.pkl', 'rb') as file:\n",
    "    # Load and deserialize the data from the file\n",
    "    data = pickle.load(file)\n",
    "\n",
    "    # Extracting the results and last iteration\n",
    "    results = data['results']\n",
    "\n",
    "for i in range(24366, 24376):\n",
    "    with open(f\"results_start_{i}_stride_10.pkl\", 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        results += data['results']\n",
    "print(len(results))\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 6197963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'pos', 'neg'],\n",
      "        num_rows: 176430\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['query', 'pos', 'neg'],\n",
      "    num_rows: 6197963\n",
      "})\n",
      "45000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "cite_prediction_new = datasets.load_dataset(\"allenai/scirepeval\", \"cite_prediction_new\")\n",
    "print(cite_prediction_new)\n",
    "print(cite_prediction_new['train'])\n",
    "from random import sample\n",
    "import random\n",
    "random.seed(42)\n",
    "percentage = 10\n",
    "size = 50000\n",
    "dataset_size = len(cite_prediction_new['train'])\n",
    "sampled_indices = sample(range(dataset_size), int(size*(1-percentage/100)))\n",
    "print(len(sampled_indices))\n",
    "# Creating a new dataset with the sampled indices\n",
    "sampled_dataset = cite_prediction_new['train'].select(sampled_indices)\n",
    "mixed_dataset = list(sampled_dataset) + sample(results, round(size*percentage/100))\n",
    "random.shuffle(mixed_dataset)\n",
    "len(mixed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/andrewm/store/hf/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30f7fa673384ddebaf8cac66e0238de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536b678606fa4ea5830128be2638bf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/45 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae519a9e53b4abeb6c1ee011c7730ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d231ad839949ebb09540e69c5f27ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "access_token_write = \"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\"\n",
    "login(token = access_token_write)\n",
    "\n",
    "# Assuming `data` is your list of dictionaries\n",
    "dataset = Dataset.from_list(mixed_dataset)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)  # Adjust the test_size as needed\n",
    "\n",
    "combined_dataset = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']  \n",
    "})\n",
    "\n",
    "combined_dataset.push_to_hub(f\"cheafdevo56/InfluentialTriplets{percentage}Percent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/andrewm/store/hf/huggingface/token\n",
      "Login successful\n",
      "<module 'datasets' from '/home/andrewm/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_write = \"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\"\n",
    "login(token = access_token_write)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"cheafdevo56/influential_citations_triplets\")\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Iteration: 240\n",
      "Total Entries: 14\n",
      "Last Iteration: 101\n",
      "Total Entries: 5\n",
      "Last Iteration: 192\n",
      "Total Entries: 5\n",
      "Last Iteration: 343\n",
      "Total Entries: 13\n",
      "Last Iteration: 924\n",
      "Total Entries: 35\n",
      "Last Iteration: 45\n",
      "Total Entries: 3\n",
      "Total 75\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "total_entries = 0\n",
    "for i in range(0, 6):\n",
    "    with open(f\"new_queries_start_{i}_stride_10_hard_False.pkl\", 'rb') as file:\n",
    "        # Load and deserialize the data from the file\n",
    "        data = pickle.load(file)\n",
    "\n",
    "        # Extracting the results and last iteration\n",
    "        results = data['results']\n",
    "        last_iteration = data['last_iteration']\n",
    "        total_entries += len(results)\n",
    "\n",
    "        # Print the retrieved data\n",
    "        print(f\"Last Iteration: {last_iteration}\")\n",
    "        print(f\"Total Entries: {len(results)}\")\n",
    "for i in range(7, 10):\n",
    "    with open(f\"new_queries_start_{i}_stride_10_hard_True.pkl\", 'rb') as file:\n",
    "        # Load and deserialize the data from the file\n",
    "        data = pickle.load(file)\n",
    "\n",
    "        # Extracting the results and last iteration\n",
    "        results = data['results']\n",
    "        last_iteration = data['last_iteration']\n",
    "        total_entries += len(results)\n",
    "\n",
    "        # Print the retrieved data\n",
    "        print(f\"Last Iteration: {last_iteration}\")\n",
    "        print(f\"Total Entries: {len(results)}\")\n",
    "print(\"Total\", total_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
